---
title: "LangChainå­¦ä¹ ç¬”è®°"
date: 2026-02-06
tags: [langchain, ai, llm, python, chains, agents]
category: AIæ¡†æ¶
status: è¿›è¡Œä¸­
difficulty: ä¸­çº§
estimated_time: "3-4å‘¨"
last_updated: 2026-02-06
version: "0.1"
---

# LangChainå­¦ä¹ ç¬”è®°

## ğŸ“– ç›®å½•
- [æ¦‚è¿°](#æ¦‚è¿°)
- [æ ¸å¿ƒæ¦‚å¿µ](#æ ¸å¿ƒæ¦‚å¿µ)
- [å®‰è£…ä¸é…ç½®](#å®‰è£…ä¸é…ç½®)
- [åŸºç¡€ç»„ä»¶](#åŸºç¡€ç»„ä»¶)
- [é“¾å¼æ“ä½œ](#é“¾å¼æ“ä½œ)
- [æç¤ºè¯æ¨¡æ¿](#æç¤ºè¯æ¨¡æ¿)
- [æ¨¡å‹é›†æˆ](#æ¨¡å‹é›†æˆ)
- [å‘é‡å­˜å‚¨](#å‘é‡å­˜å‚¨)
- [ä»£ç†ä¸å·¥å…·](#ä»£ç†ä¸å·¥å…·)
- [å®é™…åº”ç”¨ç¤ºä¾‹](#å®é™…åº”ç”¨ç¤ºä¾‹)
- [æœ€ä½³å®è·µ](#æœ€ä½³å®è·µ)
- [å¸¸è§é—®é¢˜](#å¸¸è§é—®é¢˜)

---

## ğŸ¯ æ¦‚è¿°

LangChainæ˜¯ä¸€ä¸ªç”¨äºå¼€å‘ç”±è¯­è¨€æ¨¡å‹é©±åŠ¨çš„åº”ç”¨ç¨‹åºçš„å¼ºå¤§æ¡†æ¶ã€‚å®ƒæ—¨åœ¨å¸®åŠ©å¼€å‘è€…å¿«é€Ÿæ„å»ºå¤æ‚çš„AIåº”ç”¨ï¼Œæ”¯æŒå¤šç§è¯­è¨€æ¨¡å‹å’Œå¤–éƒ¨æ•°æ®æºã€‚

### ä¸»è¦ç‰¹æ€§
- **æ¨¡å—åŒ–è®¾è®¡**ï¼šæ¾è€¦åˆçš„ç»„ä»¶ï¼Œæ˜“äºç»„åˆå’Œæ‰©å±•
- **å¤šæ¨¡å‹æ”¯æŒ**ï¼šOpenAIã€Anthropicã€Hugging Faceç­‰
- **ä¸°å¯Œçš„æ•°æ®é›†æˆ**ï¼šæ–‡æ¡£åŠ è½½ã€å‘é‡æ•°æ®åº“ã€æœç´¢å·¥å…·
- **å¼ºå¤§çš„é“¾å¼æ“ä½œ**ï¼šå°†å¤šä¸ªæ­¥éª¤ç»„åˆæˆå¤æ‚çš„AIå·¥ä½œæµ
- **ä»£ç†ç³»ç»Ÿ**ï¼šè®©AIèƒ½å¤Ÿä½¿ç”¨å·¥å…·å’Œè¿›è¡Œæ¨ç†

---

## ğŸ—ï¸ æ ¸å¿ƒæ¦‚å¿µ

### 1. Modelsï¼ˆæ¨¡å‹ï¼‰
LangChainæ”¯æŒå¤šç§ç±»å‹çš„æ¨¡å‹ï¼š
- **LLMs**ï¼šå¤§è¯­è¨€æ¨¡å‹ï¼ˆå¦‚GPTã€Claudeï¼‰
- **Chat Models**ï¼šå¯¹è¯æ¨¡å‹
- **Embedding Models**ï¼šåµŒå…¥æ¨¡å‹

### 2. Promptsï¼ˆæç¤ºè¯ï¼‰
ç®¡ç†å’Œä¼˜åŒ–å‘é€ç»™æ¨¡å‹çš„æç¤ºè¯ï¼š
- Prompt Templates
- Few-shot examples
- Output parsers

### 3. Chainsï¼ˆé“¾ï¼‰
å°†å¤šä¸ªç»„ä»¶ä¸²è”èµ·æ¥å½¢æˆå®Œæ•´çš„åº”ç”¨é€»è¾‘ï¼š
- Simple chains
- Sequential chains
- Custom chains

### 4. Retrieversï¼ˆæ£€ç´¢å™¨ï¼‰
ä»å¤–éƒ¨æ•°æ®æºæ£€ç´¢ç›¸å…³ä¿¡æ¯ï¼š
- Document loaders
- Text splitters
- Vector stores

### 5. Agentsï¼ˆä»£ç†ï¼‰
è®©æ¨¡å‹èƒ½å¤Ÿè‡ªä¸»å†³ç­–å’Œä½¿ç”¨å·¥å…·ï¼š
- ReAct agents
- Plan-and-execute agents
- Custom agents

---

## âš™ï¸ å®‰è£…ä¸é…ç½®

### åŸºæœ¬å®‰è£…
```bash
pip install langchain
```

### å®Œæ•´å®‰è£…
```bash
pip install langchain[all]
```

### ç¯å¢ƒå˜é‡é…ç½®
```bash
# .envæ–‡ä»¶
OPENAI_API_KEY=your_openai_api_key
ANTHROPIC_API_KEY=your_anthropic_api_key
```

### å¯¼å…¥æµ‹è¯•
```python
from langchain.llms import OpenAI
from langchain.chat_models import ChatOpenAI

# æµ‹è¯•åŸºæœ¬åŠŸèƒ½
llm = OpenAI(temperature=0.7)
response = llm("ä½ å¥½ï¼Œè¯·ä»‹ç»ä¸€ä¸‹LangChain")
print(response)
```

---

## ğŸ§© åŸºç¡€ç»„ä»¶

### LLMæ¥å£
```python
from langchain.llms import OpenAI
from langchain.llms import Anthropic

# OpenAI LLM
llm = OpenAI(
    model_name="gpt-3.5-turbo",
    temperature=0.7,
    max_tokens=1000
)

# Anthropic LLM
claude = Anthropic(
    model="claude-2",
    temperature=0.7
)
```

### Chat Modelæ¥å£
```python
from langchain.chat_models import ChatOpenAI
from langchain.schema import HumanMessage, SystemMessage

chat_model = ChatOpenAI(temperature=0.7)

messages = [
    SystemMessage(content="ä½ æ˜¯ä¸€ä¸ªæœ‰ç”¨çš„AIåŠ©æ‰‹"),
    HumanMessage(content="è¯·è§£é‡Šä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ")
]

response = chat_model(messages)
print(response.content)
```

### Embeddingæ¨¡å‹
```python
from langchain.embeddings import OpenAIEmbeddings

embeddings = OpenAIEmbeddings()

# ç”Ÿæˆæ–‡æœ¬åµŒå…¥
text = "LangChainæ˜¯ä¸€ä¸ªå¼ºå¤§çš„AIæ¡†æ¶"
vector = embeddings.embed_query(text)

# ç”Ÿæˆæ–‡æ¡£åµŒå…¥
documents = ["æ–‡æ¡£1", "æ–‡æ¡£2", "æ–‡æ¡£3"]
doc_vectors = embeddings.embed_documents(documents)
```

---

## ğŸ”— é“¾å¼æ“ä½œ

### ç®€å•é“¾
```python
from langchain.llms import OpenAI
from langchain.prompts import PromptTemplate
from langchain.chains import LLMChain

# åˆ›å»ºLLM
llm = OpenAI(temperature=0.7)

# åˆ›å»ºæç¤ºè¯æ¨¡æ¿
prompt = PromptTemplate(
    input_variables=["topic"],
    template="è¯·ç”¨100å­—è§£é‡Š{topic}"
)

# åˆ›å»ºé“¾
chain = LLMChain(llm=llm, prompt=prompt)

# è¿è¡Œé“¾
result = chain.run(topic="äººå·¥æ™ºèƒ½")
print(result)
```

### é¡ºåºé“¾
```python
from langchain.chains import SimpleSequentialChain

# ç¬¬ä¸€æ­¥ï¼šç”Ÿæˆäº§å“æè¿°
first_prompt = PromptTemplate(
    input_variables=["product"],
    template="ä¸º{product}ç”Ÿæˆä¸€ä¸ªå¸å¼•äººçš„æ ‡é¢˜"
)

# ç¬¬äºŒæ­¥ï¼šç”Ÿæˆè¥é”€æ–‡æ¡ˆ
second_prompt = PromptTemplate(
    input_variables=["title"],
    template="åŸºäºè¿™ä¸ªæ ‡é¢˜{title}ï¼Œå†™ä¸€æ®µè¥é”€æ–‡æ¡ˆ"
)

# åˆ›å»ºé“¾
chain1 = LLMChain(llm=llm, prompt=first_prompt)
chain2 = LLMChain(llm=llm, prompt=second_prompt)
overall_chain = SimpleSequentialChain(
    chains=[chain1, chain2],
    verbose=True
)

# è¿è¡Œ
result = overall_chain.run("æ™ºèƒ½æ‰‹è¡¨")
```

---

## ğŸ“ æç¤ºè¯æ¨¡æ¿

### åŸºç¡€æ¨¡æ¿
```python
from langchain.prompts import PromptTemplate

template = """
ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„{role}ã€‚

ä»»åŠ¡ï¼š{task}
è¦æ±‚ï¼š{requirements}

è¯·æä¾›è¯¦ç»†çš„è§£ç­”ã€‚
"""

prompt = PromptTemplate(
    input_variables=["role", "task", "requirements"],
    template=template
)

# æ ¼å¼åŒ–æç¤ºè¯
formatted_prompt = prompt.format(
    role="æŠ€æœ¯é¡¾é—®",
    task="è§£é‡ŠåŒºå—é“¾æŠ€æœ¯",
    requirements="ç”¨é€šä¿—æ˜“æ‡‚çš„è¯­è¨€ï¼Œé€‚åˆåˆå­¦è€…"
)
```

### Few-shotç¤ºä¾‹
```python
from langchain.prompts import FewShotPromptTemplate

examples = [
    {
        "input": "ä»€ä¹ˆæ˜¯AIï¼Ÿ",
        "output": "äººå·¥æ™ºèƒ½ï¼ˆAIï¼‰æ˜¯è®¡ç®—æœºç§‘å­¦çš„ä¸€ä¸ªåˆ†æ”¯ï¼Œè‡´åŠ›äºåˆ›å»ºèƒ½å¤Ÿæ‰§è¡Œé€šå¸¸éœ€è¦äººç±»æ™ºèƒ½çš„ä»»åŠ¡çš„ç³»ç»Ÿï¼Œå¦‚è§†è§‰æ„ŸçŸ¥ã€è¯­éŸ³è¯†åˆ«ã€å†³ç­–åˆ¶å®šå’Œè¯­è¨€ç¿»è¯‘ã€‚"
    },
    {
        "input": "ä»€ä¹ˆæ˜¯MLï¼Ÿ",
        "output": "æœºå™¨å­¦ä¹ ï¼ˆMLï¼‰æ˜¯äººå·¥æ™ºèƒ½çš„ä¸€ä¸ªå­é›†ï¼Œå®ƒä½¿è®¡ç®—æœºèƒ½å¤Ÿåœ¨æ²¡æœ‰æ˜ç¡®ç¼–ç¨‹çš„æƒ…å†µä¸‹å­¦ä¹ å’Œæ”¹è¿›ï¼Œé€šè¿‡ç®—æ³•ä»æ•°æ®ä¸­è¯†åˆ«æ¨¡å¼å¹¶åšå‡ºé¢„æµ‹æˆ–å†³ç­–ã€‚"
    }
]

example_prompt = PromptTemplate(
    input_variables=["input", "output"],
    template="é—®é¢˜ï¼š{input}\nå›ç­”ï¼š{output}"
)

few_shot_prompt = FewShotPromptTemplate(
    examples=examples,
    example_prompt=example_prompt,
    prefix="ä»¥ä¸‹æ˜¯ä¸€äº›é—®é¢˜å’Œå¯¹åº”çš„è¯¦ç»†å›ç­”ç¤ºä¾‹ï¼š\n",
    suffix="\nç°åœ¨è¯·å›ç­”è¿™ä¸ªé—®é¢˜ï¼š{input}",
    input_variables=["input"]
)
```

---

## ğŸ¤– æ¨¡å‹é›†æˆ

### OpenAIé›†æˆ
```python
from langchain.llms import OpenAI
from langchain.chat_models import ChatOpenAI
from langchain.embeddings import OpenAIEmbeddings

# LLM
llm = OpenAI(
    model_name="gpt-3.5-turbo-instruct",
    temperature=0.9,
    max_tokens=1024
)

# Chat Model
chat = ChatOpenAI(
    model_name="gpt-3.5-turbo",
    temperature=0.7
)

# Embeddings
embeddings = OpenAIEmbeddings(
    model_name="text-embedding-ada-002"
)
```

### Anthropicé›†æˆ
```python
from langchain.llms import Anthropic
from langchain.chat_models import ChatAnthropic

# LLM
claude_llm = Anthropic(
    model="claude-2",
    temperature=0.7,
    max_tokens=1000
)

# Chat Model
claude_chat = ChatAnthropic(
    model="claude-2",
    temperature=0.7
)
```

### Hugging Faceé›†æˆ
```python
from langchain.llms import HuggingFacePipeline
from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline

model_id = "microsoft/DialoGPT-medium"
tokenizer = AutoTokenizer.from_pretrained(model_id)
model = AutoModelForCausalLM.from_pretrained(model_id)

pipe = pipeline(
    "text-generation",
    model=model,
    tokenizer=tokenizer,
    max_length=512,
    temperature=0.7
)

llm = HuggingFacePipeline(pipeline=pipe)
```

---

## ğŸ’¾ å‘é‡å­˜å‚¨

### Chromaå‘é‡å­˜å‚¨
```python
from langchain.vectorstores import Chroma
from langchain.embeddings import OpenAIEmbeddings
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.document_loaders import TextLoader

# åŠ è½½æ–‡æ¡£
loader = TextLoader("document.txt")
documents = loader.load()

# æ–‡æœ¬åˆ†å‰²
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=1000,
    chunk_overlap=200
)
splits = text_splitter.split_documents(documents)

# åˆ›å»ºå‘é‡å­˜å‚¨
embeddings = OpenAIEmbeddings()
vectorstore = Chroma.from_documents(
    documents=splits,
    embedding=embeddings
)

# ç›¸ä¼¼æ€§æœç´¢
docs = vectorstore.similarity_search("AIæŠ€æœ¯", k=3)
print(docs[0].page_content)
```

### FAISSå‘é‡å­˜å‚¨
```python
from langchain.vectorstores import FAISS

# åˆ›å»ºå‘é‡å­˜å‚¨
vectorstore = FAISS.from_documents(
    documents=splits,
    embedding=embeddings
)

# ä¿å­˜åˆ°æœ¬åœ°
vectorstore.save_local("faiss_index")

# ä»æœ¬åœ°åŠ è½½
loaded_vectorstore = FAISS.load_local("faiss_index", embeddings)
```

### æ–‡æ¡£æ£€ç´¢é“¾
```python
from langchain.chains import RetrievalQA
from langchain.llms import OpenAI

# åˆ›å»ºæ£€ç´¢QAé“¾
qa_chain = RetrievalQA.from_chain_type(
    llm=llm,
    chain_type="stuff",
    retriever=vectorstore.as_retriever(search_kwargs={"k": 3})
)

# æ‰§è¡Œé—®ç­”
query = "LangChainçš„ä¸»è¦åŠŸèƒ½æ˜¯ä»€ä¹ˆï¼Ÿ"
result = qa_chain.run(query)
print(result)
```

---

## ğŸ› ï¸ ä»£ç†ä¸å·¥å…·

### åŸºç¡€ä»£ç†
```python
from langchain.agents import initialize_agent, Tool
from langchain.llms import OpenAI

# å®šä¹‰å·¥å…·
def calculator(expression):
    """æ‰§è¡Œæ•°å­¦è®¡ç®—"""
    try:
        return eval(expression)
    except:
        return "è®¡ç®—é”™è¯¯"

tools = [
    Tool(
        name="Calculator",
        func=calculator,
        description="ç”¨äºæ‰§è¡Œæ•°å­¦è®¡ç®—"
    )
]

# åˆ›å»ºä»£ç†
llm = OpenAI(temperature=0)
agent = initialize_agent(
    tools,
    llm,
    agent="zero-shot-react-description",
    verbose=True
)

# è¿è¡Œä»£ç†
result = agent.run("è®¡ç®—(15 * 8) + 42")
print(result)
```

### æœç´¢å·¥å…·
```python
from langchain.utilities import SerpAPIWrapper
from langchain.agents import Tool

# Googleæœç´¢
serpapi = SerpAPIWrapper()
search_tool = Tool(
    name="Google Search",
    func=serpapi.run,
    description="ç”¨äºæœç´¢æœ€æ–°ä¿¡æ¯"
)

# ä½¿ç”¨æœç´¢å·¥å…·çš„ä»£ç†
agent_with_search = initialize_agent(
    [search_tool],
    llm,
    agent="zero-shot-react-description",
    verbose=True
)

result = agent_with_search.run("LangChainçš„æœ€æ–°ç‰ˆæœ¬æ˜¯ä»€ä¹ˆï¼Ÿ")
```

### è‡ªå®šä¹‰å·¥å…·
```python
from langchain.tools import BaseTool
from typing import Type
from pydantic import BaseModel, Field

class WeatherInput(BaseModel):
    location: str = Field(description="åŸå¸‚åç§°")

class WeatherTool(BaseTool):
    name = "Weather Checker"
    description = "è·å–æŒ‡å®šåŸå¸‚çš„å¤©æ°”ä¿¡æ¯"
    args_schema: Type[BaseModel] = WeatherInput

    def _run(self, location: str) -> str:
        # è¿™é‡Œå¯ä»¥è°ƒç”¨çœŸå®çš„å¤©æ°”API
        weather_data = f"{location}çš„å¤©æ°”ï¼šæ™´æœ—ï¼Œ25Â°C"
        return weather_data

# åˆ›å»ºå·¥å…·å®ä¾‹
weather_tool = WeatherTool()

# åœ¨ä»£ç†ä¸­ä½¿ç”¨
tools = [weather_tool]
agent = initialize_agent(tools, llm, verbose=True)
```

---

## ğŸ’¡ å®é™…åº”ç”¨ç¤ºä¾‹

### 1. æ–‡æ¡£é—®ç­”ç³»ç»Ÿ
```python
from langchain.chains import RetrievalQA
from langchain.document_loaders import PyPDFLoader
from langchain.vectorstores import Chroma
from langchain.embeddings import OpenAIEmbeddings
from langchain.text_splitter import RecursiveCharacterTextSplitter

def create_document_qa_system(pdf_path):
    # åŠ è½½PDFæ–‡æ¡£
    loader = PyPDFLoader(pdf_path)
    documents = loader.load()

    # æ–‡æœ¬åˆ†å‰²
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=1000,
        chunk_overlap=200
    )
    splits = text_splitter.split_documents(documents)

    # åˆ›å»ºå‘é‡å­˜å‚¨
    embeddings = OpenAIEmbeddings()
    vectorstore = Chroma.from_documents(
        documents=splits,
        embedding=embeddings
    )

    # åˆ›å»ºQAé“¾
    qa_chain = RetrievalQA.from_chain_type(
        llm=llm,
        chain_type="stuff",
        retriever=vectorstore.as_retriever(search_kwargs={"k": 4})
    )

    return qa_chain

# ä½¿ç”¨ç¤ºä¾‹
qa_system = create_document_qa_system("document.pdf")
answer = qa_system.run("æ–‡æ¡£ä¸­çš„ä¸»è¦ç»“è®ºæ˜¯ä»€ä¹ˆï¼Ÿ")
```

### 2. æ™ºèƒ½å®¢æœæœºå™¨äºº
```python
from langchain.prompts import PromptTemplate
from langchain.chains import ConversationChain
from langchain.memory import ConversationBufferMemory

class CustomerServiceBot:
    def __init__(self):
        self.llm = ChatOpenAI(temperature=0.7)
        self.memory = ConversationBufferMemory()

        prompt = PromptTemplate(
            input_variables=["history", "input"],
            template="""
            ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„å®¢æœä»£è¡¨ã€‚è¯·æ ¹æ®å¯¹è¯å†å²å’Œå®¢æˆ·é—®é¢˜æä¾›æœ‰ç”¨ã€å‹å¥½çš„å›ç­”ã€‚

            å¯¹è¯å†å²ï¼š{history}
            å®¢æˆ·é—®é¢˜ï¼š{input}

            è¯·æä¾›è¯¦ç»†ã€ä¸“ä¸šçš„å›ç­”ï¼š
            """
        )

        self.conversation = ConversationChain(
            llm=self.llm,
            memory=self.memory,
            prompt=prompt,
            verbose=True
        )

    def ask(self, question):
        return self.conversation.predict(input=question)

    def get_history(self):
        return self.memory.buffer

# ä½¿ç”¨ç¤ºä¾‹
bot = CustomerServiceBot()
response = bot.ask("æˆ‘çš„è®¢å•ä»€ä¹ˆæ—¶å€™èƒ½å‘è´§ï¼Ÿ")
```

### 3. å†…å®¹ç”Ÿæˆæµæ°´çº¿
```python
from langchain.chains import SequentialChain
from langchain.prompts import PromptTemplate

class ContentPipeline:
    def __init__(self):
        self.llm = OpenAI(temperature=0.7)

        # ç¬¬ä¸€æ­¥ï¼šç”Ÿæˆå¤§çº²
        outline_template = """
        ä¸ºä¸»é¢˜"{topic}"ç”Ÿæˆä¸€ä¸ªè¯¦ç»†çš„æ–‡ç« å¤§çº²ï¼ŒåŒ…å«5-7ä¸ªä¸»è¦ç« èŠ‚ã€‚
        """
        self.outline_prompt = PromptTemplate(
            input_variables=["topic"],
            template=outline_template
        )

        # ç¬¬äºŒæ­¥ï¼šç”Ÿæˆå†…å®¹
        content_template = """
        åŸºäºä»¥ä¸‹å¤§çº²ï¼Œç”Ÿæˆä¸€ç¯‡è¯¦ç»†çš„æ–‡ç« ï¼š

        {outline}

        è¦æ±‚ï¼š
        - æ¯ä¸ªç« èŠ‚è‡³å°‘200å­—
        - ä½¿ç”¨ç”ŸåŠ¨çš„ä¾‹å­
        - è¯­è¨€é€šä¿—æ˜“æ‡‚
        """
        self.content_prompt = PromptTemplate(
            input_variables=["outline"],
            template=content_template
        )

        # åˆ›å»ºé¡ºåºé“¾
        self.outline_chain = LLMChain(
            llm=self.llm,
            prompt=self.outline_prompt,
            output_key="outline"
        )

        self.content_chain = LLMChain(
            llm=self.llm,
            prompt=self.content_prompt,
            output_key="content"
        )

        self.pipeline = SequentialChain(
            chains=[self.outline_chain, self.content_chain],
            input_variables=["topic"],
            output_variables=["outline", "content"],
            verbose=True
        )

    def generate_content(self, topic):
        result = self.pipeline({"topic": topic})
        return result

# ä½¿ç”¨ç¤ºä¾‹
pipeline = ContentPipeline()
article = pipeline.generate_content("äººå·¥æ™ºèƒ½çš„å‘å±•å†ç¨‹")
print("å¤§çº²ï¼š", article["outline"])
print("å†…å®¹ï¼š", article["content"])
```

---

## âœ… æœ€ä½³å®è·µ

### 1. æç¤ºè¯ä¼˜åŒ–
- **æ˜ç¡®å…·ä½“çš„æŒ‡ä»¤**ï¼šæä¾›æ¸…æ™°çš„æŒ‡å¯¼
- **ä½¿ç”¨ç¤ºä¾‹**ï¼šfew-shotå­¦ä¹ æé«˜æ•ˆæœ
- **ç»“æ„åŒ–è¾“å‡º**ï¼šæŒ‡å®šè¿”å›æ ¼å¼
- **é”™è¯¯å¤„ç†**ï¼šå¤„ç†è¾¹ç•Œæƒ…å†µ

```python
# å¥½çš„æç¤ºè¯ç¤ºä¾‹
template = """
ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æŠ€æœ¯æ–‡æ¡£å†™ä½œè€…ã€‚

ä»»åŠ¡ï¼šä¸º{product}ç¼–å†™äº§å“è¯´æ˜æ–‡æ¡£

è¦æ±‚ï¼š
1. åŒ…å«äº§å“æ¦‚è¿°ã€æ ¸å¿ƒåŠŸèƒ½ã€æŠ€æœ¯è§„æ ¼
2. ä½¿ç”¨æ¸…æ™°çš„ç« èŠ‚ç»“æ„
3. åŒ…å«å®é™…ä½¿ç”¨ç¤ºä¾‹
4. å­—æ•°æ§åˆ¶åœ¨500-800å­—

äº§å“ä¿¡æ¯ï¼š
{product_info}

è¯·æŒ‰ç…§ä»¥ä¸Šè¦æ±‚ç”Ÿæˆæ–‡æ¡£ï¼š
"""

prompt = PromptTemplate(
    input_variables=["product", "product_info"],
    template=template
)
```

### 2. é“¾è®¾è®¡åŸåˆ™
- **å•ä¸€èŒè´£**ï¼šæ¯ä¸ªé“¾åªè´Ÿè´£ä¸€ä¸ªç‰¹å®šä»»åŠ¡
- **å¯ç»„åˆæ€§**ï¼šè®¾è®¡å¯é‡ç”¨çš„ç»„ä»¶
- **é”™è¯¯å¤„ç†**ï¼šæ·»åŠ é€‚å½“çš„å¼‚å¸¸å¤„ç†
- **æ€§èƒ½ä¼˜åŒ–**ï¼šåˆç†ä½¿ç”¨ç¼“å­˜å’Œæ‰¹å¤„ç†

```python
from langchain.callbacks import StdOutCallbackHandler
from langchain.callbacks.base import BaseCallbackHandler

class LoggingCallback(BaseCallbackHandler):
    def on_chain_start(self, serialized, inputs, **kwargs):
        print(f"å¼€å§‹æ‰§è¡Œé“¾: {serialized.get('name', 'Unknown')}")

    def on_chain_end(self, outputs, **kwargs):
        print(f"é“¾æ‰§è¡Œå®Œæˆ: {outputs}")

# ä½¿ç”¨å›è°ƒ
handler = LoggingCallback()
chain = LLMChain(llm=llm, prompt=prompt, callbacks=[handler])
```

### 3. å‘é‡å­˜å‚¨ä¼˜åŒ–
- **åˆé€‚çš„åˆ†å—ç­–ç•¥**ï¼šæ ¹æ®å†…å®¹ç±»å‹è°ƒæ•´chunk_size
- **é‡å å¤„ç†**ï¼šé€‚å½“è®¾ç½®chunk_overlapé¿å…ä¸Šä¸‹æ–‡ä¸¢å¤±
- **ç´¢å¼•ä¼˜åŒ–**ï¼šå®šæœŸæ¸…ç†å’Œé‡å»ºç´¢å¼•
- **æ‰¹é‡å¤„ç†**ï¼šå¤„ç†å¤§é‡æ–‡æ¡£æ—¶ä½¿ç”¨æ‰¹å¤„ç†

```python
# ä¼˜åŒ–çš„æ–‡æœ¬åˆ†å‰²
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=1000,
    chunk_overlap=200,
    separators=["\n\n", "\n", " ", ""],
    length_function=len
)

# æ‰¹é‡å¤„ç†
def batch_process_documents(documents, batch_size=100):
    for i in range(0, len(documents), batch_size):
        batch = documents[i:i+batch_size]
        yield text_splitter.split_documents(batch)
```

### 4. å†…å­˜ç®¡ç†
- **å¯¹è¯è®°å¿†**ï¼šåˆç†ç®¡ç†å¯¹è¯å†å²
- **ç¼“å­˜ç­–ç•¥**ï¼šç¼“å­˜å¸¸ç”¨æŸ¥è¯¢ç»“æœ
- **èµ„æºæ¸…ç†**ï¼šåŠæ—¶é‡Šæ”¾ä¸éœ€è¦çš„èµ„æº

```python
from langchain.memory import ConversationBufferWindowMemory

# çª—å£è®°å¿†ï¼ˆåªä¿ç•™æœ€è¿‘Nè½®å¯¹è¯ï¼‰
memory = ConversationBufferWindowMemory(
    k=5,  # åªä¿ç•™æœ€è¿‘5è½®å¯¹è¯
    return_messages=True
)
```

---

## â“ å¸¸è§é—®é¢˜

### Q1: å¦‚ä½•å¤„ç†APIè°ƒç”¨é™åˆ¶ï¼Ÿ
```python
import time
from langchain.llms import OpenAI

class RateLimitedLLM:
    def __init__(self, llm, max_calls_per_minute=60):
        self.llm = llm
        self.max_calls_per_minute = max_calls_per_minute
        self.call_times = []

    def __call__(self, *args, **kwargs):
        # æ¸…ç†è¶…è¿‡1åˆ†é’Ÿçš„è°ƒç”¨è®°å½•
        current_time = time.time()
        self.call_times = [
            t for t in self.call_times
            if current_time - t < 60
        ]

        # æ£€æŸ¥æ˜¯å¦è¶…è¿‡é™åˆ¶
        if len(self.call_times) >= self.max_calls_per_minute:
            sleep_time = 60 - (current_time - self.call_times[0])
            if sleep_time > 0:
                time.sleep(sleep_time)

        # è®°å½•è°ƒç”¨æ—¶é—´
        self.call_times.append(current_time)

        return self.llm(*args, **kwargs)

# ä½¿ç”¨ç¤ºä¾‹
rate_limited_llm = RateLimitedLLM(llm, max_calls_per_minute=50)
```

### Q2: å¦‚ä½•ä¼˜åŒ–å¤§æ–‡æ¡£å¤„ç†ï¼Ÿ
```python
from langchain.document_loaders import UnstructuredFileLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.chains import MapReduceDocumentsChain, ReduceDocumentsChain
from langchain.chains.combine_documents.stuff import StuffDocumentsChain
from langchain.chains.combine_documents.map_reduce import MapReduceDocumentsChain

def process_large_document(file_path):
    # åŠ è½½æ–‡æ¡£
    loader = UnstructuredFileLoader(file_path)
    docs = loader.load()

    # æ–‡æœ¬åˆ†å‰²
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=2000,
        chunk_overlap=200
    )
    splits = text_splitter.split_documents(docs)

    # Map-Reduceå¤„ç†
    map_template = """
    è¯·æ€»ç»“ä»¥ä¸‹æ–‡æ¡£ç‰‡æ®µçš„è¦ç‚¹ï¼š

    {docs}

    è¦ç‚¹æ€»ç»“ï¼š
    """
    map_prompt = PromptTemplate.from_template(map_template)

    reduce_template = """
    å°†ä»¥ä¸‹è¦ç‚¹æ€»ç»“æˆä¸€ä»½å®Œæ•´çš„æ‘˜è¦ï¼š

    {doc_summaries}

    å®Œæ•´æ‘˜è¦ï¼š
    """
    reduce_prompt = PromptTemplate.from_template(reduce_template)

    # åˆ›å»ºå¤„ç†é“¾
    map_chain = LLMChain(llm=llm, prompt=map_prompt)
    reduce_chain = LLMChain(llm=llm, prompt=reduce_prompt)

    # æœ€ç»ˆæ–‡æ¡£é“¾
    combine_documents = StuffDocumentsChain(
        llm_chain=reduce_chain,
        document_variable_name="doc_summaries"
    )

    reduce_documents = ReduceDocumentsChain(
        combine_documents=combine_documents,
        collapse_documents=combine_documents,
        token_max=4000
    )

    map_reduce_chain = MapReduceDocumentsChain(
        llm_chain=map_chain,
        reduce_documents_chain=reduce_documents,
        document_variable_name="docs"
    )

    return map_reduce_chain.run(splits)
```

### Q3: å¦‚ä½•è°ƒè¯•LangChainåº”ç”¨ï¼Ÿ
```python
import logging
from langchain.callbacks.base import BaseCallbackHandler

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.DEBUG)
logger = logging.getLogger(__name__)

class DebugCallback(BaseCallbackHandler):
    def on_chain_start(self, serialized, inputs, **kwargs):
        logger.debug(f"é“¾å¼€å§‹: {serialized}")
        logger.debug(f"è¾“å…¥: {inputs}")

    def on_chain_end(self, outputs, **kwargs):
        logger.debug(f"é“¾ç»“æŸ: {outputs}")

    def on_llm_start(self, serialized, prompts, **kwargs):
        logger.debug(f"LLMå¼€å§‹: {serialized}")
        logger.debug(f"æç¤ºè¯: {prompts}")

    def on_llm_end(self, response, **kwargs):
        logger.debug(f"LLMç»“æŸ: {response}")

# åœ¨åº”ç”¨ä¸­ä½¿ç”¨è°ƒè¯•å›è°ƒ
debug_handler = DebugCallback()
chain = LLMChain(llm=llm, prompt=prompt, callbacks=[debug_handler])
```

### Q4: å¦‚ä½•å®ç°è‡ªå®šä¹‰ä»£ç†ï¼Ÿ
```python
from langchain.agents import AgentExecutor, create_react_agent
from langchain.tools import Tool
from langchain.prompts import PromptTemplate
from langchain.schema import BaseMessage, HumanMessage, AIMessage

class CustomAgent:
    def __init__(self, llm, tools, prompt_template):
        self.llm = llm
        self.tools = {tool.name: tool for tool in tools}

        prompt = PromptTemplate.from_template(prompt_template)
        self.agent = create_react_agent(llm, tools, prompt)
        self.agent_executor = AgentExecutor(
            agent=self.agent,
            tools=tools,
            verbose=True,
            max_iterations=10
        )

    def run(self, query):
        return self.agent_executor.run(query)

# è‡ªå®šä¹‰ä»£ç†ç¤ºä¾‹
tools = [
    Tool(
        name="Weather",
        func=self.get_weather,
        description="è·å–å¤©æ°”ä¿¡æ¯"
    ),
    Tool(
        name="Calculator",
        func=self.calculate,
        description="æ‰§è¡Œè®¡ç®—"
    )
]

prompt_template = """
ä½¿ç”¨ä»¥ä¸‹å·¥å…·å›ç­”ç”¨æˆ·é—®é¢˜ã€‚

å¯ç”¨å·¥å…·: {tools}
å·¥å…·æè¿°: {tool_descriptions}

ç”¨æˆ·é—®é¢˜: {input}
{agent_scratchpad}
"""

agent = CustomAgent(llm, tools, prompt_template)
```

---

## ğŸ“š å­¦ä¹ èµ„æº

### å®˜æ–¹èµ„æº
- [LangChainå®˜æ–¹æ–‡æ¡£](https://python.langchain.com/)
- [LangChain GitHubä»“åº“](https://github.com/langchain-ai/langchain)
- [LangChain Hub](https://smith.langchain.com/)

### æ¨èæ•™ç¨‹
- LangChainå®˜æ–¹æ•™ç¨‹å’Œç¤ºä¾‹
- YouTubeä¸Šçš„LangChainå®æˆ˜æ•™ç¨‹
- å„å¤§å­¦ä¹ å¹³å°çš„LangChainè¯¾ç¨‹

### ç¤¾åŒºèµ„æº
- LangChain Discordç¤¾åŒº
- Stack Overflowä¸Šçš„LangChainæ ‡ç­¾
- Redditçš„r/LangChainç¤¾åŒº

---

## ğŸ”„ æŒç»­å­¦ä¹ 

### ç‰ˆæœ¬æ›´æ–°è¿½è¸ª
- å…³æ³¨LangChain GitHub releases
- è®¢é˜…å®˜æ–¹åšå®¢å’Œé€šè®¯
- å‚ä¸ç¤¾åŒºè®¨è®º

### å®è·µé¡¹ç›®å»ºè®®
1. æ„å»ºä¸ªäººçŸ¥è¯†åº“é—®ç­”ç³»ç»Ÿ
2. å¼€å‘å¤šæ¨¡æ€AIåº”ç”¨
3. åˆ›å»ºè‡ªåŠ¨åŒ–å·¥ä½œæµ
4. é›†æˆå¤šä¸ªæ•°æ®æºçš„æ™ºèƒ½åŠ©æ‰‹

### ä¸‹ä¸€æ­¥å­¦ä¹ æ–¹å‘
- LangGraphï¼šæ„å»ºå¤æ‚çš„å¤šæ™ºèƒ½ä½“ç³»ç»Ÿ
- LangServeï¼šéƒ¨ç½²LangChainåº”ç”¨
- LangSmithï¼šè°ƒè¯•å’Œç›‘æ§å·¥å…·
- æ›´å¤šæ¨¡å‹æä¾›å•†é›†æˆ

---

*æœ¬ç¬”è®°å°†æŒç»­æ›´æ–°ï¼Œè®°å½•LangChainçš„å­¦ä¹ è¿‡ç¨‹å’Œå®è·µç»éªŒã€‚*

**åˆ›å»ºæ—¶é—´**: 2026-02-06
**æœ€åæ›´æ–°**: 2026-02-06
**ç‰ˆæœ¬**: v0.1