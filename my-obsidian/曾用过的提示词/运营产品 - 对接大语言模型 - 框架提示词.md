你是一名 **资深 AI 系统架构师 + Python 后端工程师**。  
当前我已经有一个 **可运行的 FastAPI 后端工程**，并且：

- 文档 / 版本 / 对话等业务已完成
- 前后端联调已跑通
- 目前 **尚未真正接入任何大模型**

现在请你 **只做一件事**：

> **为该后端工程设计并落地一个“可扩展的大模型接入架构层”**

⚠️ 本阶段 **不实现任何具体业务 Prompt**  
⚠️ **不修改现有业务逻辑语义**  
⚠️ **不直接把模型调用写死在 API 或 Service 中**

---

## 一、架构设计目标（非常重要）

你要设计的是一个 **LLM Infrastructure Layer**，它必须满足：

1. **模型无关**
    - 不绑定 OpenAI / Claude 等任何模型
2. **业务无感**
    - 上层业务只关心：
        `llm.generate(prompt) -> str`
3. **可替换**
    - 未来切模型 / 多模型 / fallback 不影响业务
4. **可测试**
    - 可以轻松 mock / stub

---

## 二、允许新增的目录与文件（必须遵守）

请在现有工程中 **新增但不重构** 以下结构：

```
backend/src/app/
├── llm/
│   ├── __init__.py
│   ├── base.py          # LLM 抽象接口
│   ├── types.py         # Prompt / Response 类型定义
│   ├── registry.py     # 模型注册与选择
│   └── providers/
│       ├── __init__.py
│       ├── mock.py     # Mock LLM（用于测试）
│       └── placeholder.py  # 占位真实模型实现
```

> 如果某些目录已存在但用途不同，不要冲突  
> 新增内容应 **语义自洽**

---

## 三、核心抽象设计（必须实现）

### 1️⃣ LLM 基类（base.py）

请设计一个 **最小但正确的抽象接口**，例如：

- `generate(prompt: Prompt) -> LLMResponse`
- 同步版本即可（暂不做 streaming）

⚠️ 注意：

- **不要暴露 HTTP / SDK 细节**
- **不要让业务知道模型参数**

---

### 2️⃣ Prompt / Response 类型（types.py）

请定义清晰的结构类型，用于：

- Prompt（系统提示 / 用户输入 / 上下文）
- Response（文本、token 用量占位等）

要求：

- 类型清晰
- 可扩展
- 不和具体模型耦合

---

### 3️⃣ Provider 实现（providers/）

#### mock.py

- 固定返回一段可预测文本
- 用于：
    - 本地开发
    - 单元测试

#### placeholder.py

- 模拟一个“真实模型 Provider”
- 不要求真正调用外部 API
- 但结构要像真的（如 `call_model()`）

---

### 4️⃣ Registry / Factory（registry.py）

请实现一个 **模型选择机制**：

- 支持：
    - `get_llm("mock")`
    - `get_llm("default")`
- 默认返回 mock 或 placeholder
- 未来可扩展为：
    - 多模型
    - fallback
    - routing

---

## 四、配置方式（重要）

### config.py 中：

- 增加：
    - `LLM_PROVIDER = "mock"`（默认）
- 不引入复杂配置系统
- 不接环境变量也可以（先简单）

---

## 五、与现有业务的集成方式（只留“钩子”）

你需要做的 **仅限于**：
- 在 `llm_service.py`（如果已存在）中：
    - 改为通过 `llm.registry.get_llm()` 获取实例
- **不要**
    - 写业务 Prompt
    - 改 API 行为
    - 改返回结构

---

## 六、代码与工程要求（非常重要）

- 每个新增文件顶部写清楚：
    > “这是 LLM 架构层，不包含业务逻辑”
    
- 抽象层：
    - 简单
    - 明确
    - 不 over-design
- 不做 async / streaming（后续阶段再做）

---

## 七、验收标准（你必须确保）

> 在不接任何真实模型的情况下：

- 后端可以正常启动
- 原有 `/chat` 等接口不报错
- LLM 返回内容来自 mock / placeholder
- **业务代码中不直接出现模型 SDK / HTTP 调用**

---

## 八、工程哲学（你一定要遵守）

- **这是“能力层”，不是“功能层”**
- 现在的目标是：
    > **“让大模型接入成为一等公民，但暂时不干活”**
- 宁可简单，也不要耦合